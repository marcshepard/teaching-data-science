{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is a motivational introduction to artificial neural networks (aka <i>neural nets</i>). We'll explore what they are, why they were created, how they work, and finally we'll create one that can recongize hand-written digits with a very high degree of accuracy.\n",
    "\n",
    "First let's talk about why neural nets were created in the first place. Prior to neural nets, computers could easily outperform people at tasks such as arithmatic, but were unable to do a decent job at many tasks that people found easy such as image recognition and natural language processing. As a simple example, consider the following set of images of hand-written digits:\n",
    "<center><img src=\"images/MNIST-digits.jpg\"/></center>\n",
    "\n",
    "Each one of the images above is a 25x25 grid of light intensities. Each element in the grid is called a <i>pixel</i>, which is short for picture element. Imagine trying to write a computer program to figure out what digit each picture represents; \"if pixel at location (2,3) > minimum_intensity and ... then output 1\" (or some such). It would be impossible, right?\n",
    "\n",
    "And the problem can be even harder for more complex images:\n",
    "<center><img src=\"images/Chihuahua-or-muffin.jpg\"/></center>\n",
    "\n",
    "To solve these problems, people came up with the idea of creating computer programs that work more like a brain; instead of creating code that is specific to a given problem, they create code that models a simple brain. They then train that brain by showing it data so it learns a specific task (like reconginizing hand-written digits, or distinguishing a chihuahua from a muffin); similar to how people learn. Let's look at how this works in more detail.\n",
    "\n",
    "# Artificial Neural Networks\n",
    "\n",
    "Per the picture below, each neuron in your brain takes inputs to it's <i>dendtrites</i> and, if the input signals are high enough, fires an output to it's <i>axon</i>.\n",
    "<center><img src=\"images/Neuron.jpg\"/></center>\n",
    "\n",
    "The human brain is made up of about 100 billion interconnected neurons, each with about 1000 connections to other neurons, for about 100 trillion neural connections! The connection between one neurons output axon and another neurons input dentrite is called a <i>synaptic connection</i>. All of your memories and knowledge and skills are encoding in the stength of the synaptic connections between your neurons. Learning is just the brain creating, strengthening, or weakening the 100 trillion synaptic connections between your neurons.\n",
    "\n",
    "Pictured below is an artificial neuron, which is modeled as a set of inputs (corresponding to dentrites), a set of weights to apply to each input (corresponding to synaptic stengths), and an output (corresponding to the axon). The artificial neurons weights can be adjusted to allow it to <i>learn</i>:\n",
    "<center><img src=\"images/Artificial-neuron.jpg\"/></center>\n",
    "\n",
    "Here's a quick summary of some of the key differences between the human brain and artificial neural networks:\n",
    "<table>\n",
    "<tr>\n",
    " <th/>\n",
    " <th>Human Brain</th>\n",
    " <th>Artificial</th>\n",
    "</tr>\n",
    "<tr>\n",
    " <td>Size</td>\n",
    " <td>100 trillion synaptic connections</td>\n",
    " <td>200 billion parameters for a very large network like ChatGPT, although small neural nets like we'll build may have just a few hundred neurons.</td>\n",
    "</tr>\n",
    "<tr>\n",
    " <td>Speed</td>\n",
    " <td>Hundreds of operations per second</td>\n",
    " <td>Billions of operations per second</td>\n",
    "</tr>\n",
    "<tr>\n",
    " <td>Parallelism</td>\n",
    " <td>All signals run in parallel</td>\n",
    " <td>CPUs can only a very small number of operations at a time. High end GPUs can run millions of operations to run at a time, but only if all the operations are the same</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "On a creepy note, researchers have also experimented with creating <a href=\"https://research.ufl.edu/publications/explore/v10n1/extract2.html\">neural nets from rat brains</a> and, after a number of studies, discovered that the rats don't like these experiments. For more information, please read <a href='https://towardsdatascience.com/the-differences-between-artificial-and-biological-neural-networks-a8b46db828b7'>artifical vs biological neural networks</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducting the MNIST data set and TensorFlow\n",
    "\n",
    "We'll train a neural network to recognize hand-written digits. To do that, we'll need::\n",
    "1) A framework for creating neural networks. We'll use TensorFlow, created by Google (Meta's PyTorch is another popular framework, but is a bit more complicated).\n",
    "2) A dataset of labeled images of digits. By a labeled image, we mean both the image (a 28x28 grid of pixels) and the label (the actual number that was drawn in the image; 0, 1, ..., 9). We'll use the MNIST dataset, which contains a large set of labeled images for training the neural net, and another set of labeled images to test how the neural net performs on images it has never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code cell (and later the ones below) using the icon to the left of the cell, or by pressing Shift+Enter\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train, test = tf.keras.datasets.mnist.load_data()   # Load the MNIST dataset\n",
    "train_images, train_labels = train                  # Put the training images and labels in separate variables\n",
    "\n",
    "# Normalize the images\n",
    "train_images = train_images / 255.0\n",
    "\n",
    "# Show the first 20 images and their corresponding labels\n",
    "plt.figure(figsize=(10,10)) # Make is larger (10x10 inches) so you can see the pixels better\n",
    "for i in range(20):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(train_images[i], cmap='gray_r')      # Show the image in reverse grayscale\n",
    "    plt.xlabel(train_labels[i])                     # Show the label underneath the image\n",
    "# Add text to the top of the figure saying it's the first 20 images\n",
    "plt.suptitle(f\"The MNIST training data consists of {train_images.shape[0]} images, each of size {train_images.shape[1:]}.\\nHere are the first 20:\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a simple 3-layer neural net for processing the images:\n",
    "<center><img src=\"images/3-layer-net.jpg\"/></center>\n",
    "\n",
    "We'll use the following three layers:\n",
    "1) Input: Takes a 2d 28x28 image (28x28 grid of light intensitities) and flattens it to a 1-dimensional vector of 784 light intensities (784 = 28x28)\n",
    "2) Hidden: Takes 784 inputs from the previous layer and creates 64 outputs (so it has 64 'neurons', each getting full input from the previous layer) \n",
    "3) Output: Takes the 64 inputs and outputs 10 probabilities; the percent chance the network assigns to the image being each of the 10 possible digits)\n",
    "\n",
    "Let's create this neural network and see how well it does at recognizing the images..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28), name='flatten'),\n",
    "    tf.keras.layers.Dense(64, activation='relu', name='hidden_layer'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax', name='output_probabilities')\n",
    "], name='simple_neural_net')\n",
    "neural_net.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "def show_predictions (images, labels, model):\n",
    "    predictions = model.predict(images) # Predict the labels for each images\n",
    "\n",
    "    # Calculate the accuracy by seeing how many of the predictions were correct\n",
    "    accuracy = sum([predictions[i].argmax() == labels[i] for i in range(len(labels))]) / len(labels)\n",
    "\n",
    "    # Show the first 20 images and their corresponding predictions\n",
    "    plt.figure(figsize=(10,10)) # Make is larger (10x10 inches) so you can see the pixels better\n",
    "    for i in range(20):\n",
    "        plt.subplot(5,5,i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(images[i], cmap='gray_r')                    # Show the images in reverse grayscale (black on white)\n",
    "        plt.xlabel(f\"Prediction: {predictions[i].argmax()}\")    # Show the label underneath the image\n",
    "    plt.suptitle(f\"Model accuracy: {accuracy:.2%} of {images.shape[0]} images\\nModel predictions for the first 20 images:\")\n",
    "    plt.show()\n",
    "\n",
    "show_predictions(train_images, train_labels, neural_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow - that was stunningly bad. Why do you think that is?\n",
    "\n",
    "Well, the model is initialized with random weights for the neural connections, so it's predictions are just random guesses. So it will typically have about 10% accuracy (because there are 10 digits, there is a 1 in 10 chance that a random guess will be the right answer). Try running the cell multiple times and you will see it's accuracy will be different each time, but typically in the 5%-15% range.\n",
    "\n",
    "To make the model work better, we first need to train it. Let's try training it on the training dataset and see how it does after training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First print out a summary of all the weights (aka 'parameters') in ther neural network that are going to be trained\n",
    "print (neural_net.summary())    # You will see there are about 50,000 parameters in this neural network\n",
    "\n",
    "# Next, train the neural network for 5 epochs (5 passes through the training data)\n",
    "neural_net.fit(train_images, train_labels, epochs=5)\n",
    "\n",
    "# Now let's see how good it is at predictions after it's been trained:\n",
    "show_predictions(train_images, train_labels, neural_net)\n",
    "\n",
    "# Let's also see how good it is at predicitons on the test data set, which it has never seen before and was not trained on\n",
    "test_images, test_labels = test\n",
    "test_images = test_images / 255.0\n",
    "show_predictions(test_images, test_labels, neural_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that's pretty good; over 98% accuracy on the training images, and 97% on the test images. Some questions:\n",
    "1) Why do you think the accuracy is lower on the test images?\n",
    "2) What might you try if you want to get even higher accuracy on the test images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Create a more complex network - deeper (more layers) and wider (more neurons in the hidden layers)\n",
    "# Add 'dropout' layers to prevent overfitting the training data, which will improve test accuracy\n",
    "# Finally, train for a bit longer, until the test accuracy stops improving for several epochs in a row\n",
    "neural_net = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28), name='flatten'),\n",
    "    tf.keras.layers.Dropout(0.1, name='dropout_1'),\n",
    "    tf.keras.layers.Dense(128, activation='relu', name='hidden_layer_1'),\n",
    "    tf.keras.layers.Dropout(0.1, name='dropout_2'),\n",
    "    tf.keras.layers.Dense(64, activation='relu', name='hidden_layer_2'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax', name='output_probabilities')\n",
    "], name='more_complex_neural_net')\n",
    "neural_net.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# First print out a summary of all the weights in ther neural network that are going to be trained\n",
    "print (neural_net.summary())\n",
    "\n",
    "# Train the network longer (more epochs)\n",
    "neural_net.fit(train_images, train_labels, epochs=20)\n",
    "\n",
    "# Let's see if that improves the test accuracy\n",
    "show_predictions(test_images, test_labels, neural_net)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
