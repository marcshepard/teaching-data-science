{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In intro-to-neural-nets, we learned what neural nets are and trained a simple one to recongize hand-written digits. In this notebook, we'll go into a little more depth into the math that powers the training. This notebook assumes you know a little calculus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Overview\n",
    "\n",
    "Recall once we had our MNIS training data (input images + the associated digits), there were just a few lines of code to create and train a simple three-layer neural net that could recognize these digits with about 98% accuracy:\n",
    "\n",
    "```python\n",
    "neural_net = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),          # Input layer; takes in a 28x28 pixel image, flattens it.\n",
    "    tf.keras.layers.Dense(64, activation='relu'),           # Hidden layer; 64 neurons, with ReLU activation\n",
    "    tf.keras.layers.Dense(10, activation='softmax')         # Output layer; 10 neurons, with softmax activation\n",
    "])\n",
    "neural_net.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "neural_net.fit(train_images, train_labels, epochs=5)\n",
    "```\n",
    "\n",
    "There is a lot of math to unpack in how this works, so let's delve into it. We'll start with the first 4 lines (the neural net itself, which starts with random weights and so will make random predictions) before jumping into the last two lines (training it so it makes good predictions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainable parameters\n",
    "Recall that each neuron outputs some linear combination of it's inputs. In particular:\n",
    "* It takes n inputs: x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>n</sub>\n",
    "* It produces one output: x<sub>1</sub>\\*w<sub>1</sub> + ... + x<sub>n</sub>\\*w<sub>n</sub> + b\n",
    "* It has n+1 trainable parameters: n weights (w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>n</sub>) and a bias b\n",
    "<center><img src=\"images/Artificial-neuron.png\"/></center>\n",
    "\n",
    "It's easier to use vector notation: X\\*W + b, where X and W are n-dimensional vectors and \\* is the dot-product.\n",
    "\n",
    "In fact, it's even more efficient to use [matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication), where the matrix's columns are weight vectors for each neuron in that layer. GPUs can then compute the entire layers output in a single operation (rather than having a CPU do n*m multiplication operations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Activation functions\n",
    "Each layer of neurons performs a linear function on it's input (matrix multiplication). But chaining together layers of linear functions is pointless because:\n",
    "1. If f(X) and g(X) are linear functions, then so is f(g(X)).\n",
    "2. So if a neural net has 2 inputs and 1 output and only does linear processing, then it doesn't matter how many hidden layers it has or how wide each layer is; even if it has 5 layers and a million parameters, it will be equivalent to a neural net consisting of just 1 neuron with 2 inputs and 3 parameters (that is; a function of the form f(x, y) = a\\*x + b\\*y + c)\n",
    "\n",
    "To make neural nets capable of more than computing simple linear functions, we need to put something between the linear layers. That something is called an activation function.\n",
    "<center><img src=\"images/neural-layer-with-activation.png\"/></center>\n",
    "\n",
    "Let's look at a few important activation functions:\n",
    "<center><img src=\"images/activation_functions.png\"/></center>\n",
    "\n",
    "To the left is the human brain's activation function; it is a threshold/step function that outputs a fixed signal only if the weighted inputs are above a threshold. The step function doesn't work well for artificial neural nets because the training algorithm, which we'll talk about shortly, won't work if the derivative is zero everwhere.\n",
    "\n",
    "The right three are the most commonly used activation functions in artificial neural nets:\n",
    "\n",
    "* [**ReLU**](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) - typical activation between layers; super simple and surprisingly effective.\n",
    "* [**sigmoid**](https://en.wikipedia.org/wiki/Sigmoid_function) - typical output activation for binary classification networks (e.g, output is a single probability; true/false, cancer/not cancer, etc). It takes a number and outputs a number between 0 and 1. If the output is > 50%, it predicts positive. \n",
    "* [**softmax**](https://en.wikipedia.org/wiki/Softmax_function) - typical output activation for multi-class classifiers (e.g, which of the 10 digits is it). It takes n numbers and outputs n probabilities that sum is 1. The models prediction is the output with the highest probability.\n",
    "\n",
    "OK; we are now in a position to understand the 4 lines of code that build our simple neural net. Run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "neural_net = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28), name=\"input\"),    # 28x28 2D input image => 784 1D output vector \n",
    "    tf.keras.layers.Dense(64, activation='relu', name=\"hidden\"),    # 784 dim input => 64 dim output (matrix multiplication + ReLU activation)\n",
    "    tf.keras.layers.Dense(10, activation='softmax', name=\"output\")  # 64 dim => 10 propabilities (matrix multiplication + softmax activation)\n",
    "])\n",
    "neural_net.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some questions before we continue:\n",
    "1. How many trainable parameters does the input layer have? So will it get better with training?\n",
    "2. Why does the hidden layer have 50240 trainable parameters?\n",
    "3. Why does the output layer have 650 trainable parameters?\n",
    "4. Why was the ReLU activation function used in the hidden layer (vs no activation function)?\n",
    "5. Why was the softmax activation function used in the output layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "We've build a neural net, but initially the weights are all random, so we next need to train it. Training a neural net consists of iteratively performing two steps:\n",
    "1. A **forward pass**: Giving the network a batch of training input, and seeing how close it's predicted output is to the expected output.\n",
    "2. A **backwards pass**: Adjusting the network parameters slightly so the next forward pass will be closer\n",
    "\n",
    "In order to perform the first step, we need a function to measure how close the models predictions are from the expected value. That is called a *loss function*.\n",
    "* It takes two inputs; the expected and predicted values\n",
    "* It outputs one value; a number indicating how close the two are (0 = a perfect match)\n",
    "\n",
    "Common loss functions include:\n",
    "* [**Mean square error**](https://en.wikipedia.org/wiki/Mean_squared_error) - used for regression models (models that output a number), like predicting the price of a home.\n",
    "* [**Cross entropy (aka log loss)**](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a) - used for classification models (models that output probabilities; typically using sigmoid or softmax output activations), such as cancer/not cancer, which digit was drawn, etc. You could use MSE for these too, but this function is better at heavily penalizing high probability predictions that are wrong (which will help during gradient descent, our next topic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "At this point, we've:\n",
    "* Obtained some training data (inputs, expected outputs)\n",
    "* Built a neural net with trainable parameters\n",
    "* Defined a loss function to measure how far off the neural nets predictions are on the training data after performing a *forward pass* on a batch of training data\n",
    "\n",
    "Next up is adjusting the parameters (training them) in order to minize the loss function (so that the model's predicted output is closer to the expected output). Of course we know from calculus the minimum occurs when the derivative is zero. But unlike the simple equations you get in a first year calculus class, the neural net equations are way too complicated to solve in closed form.\n",
    "\n",
    "For each batch of training data we process in a forward pass, we can compute the derivative of the loss function with respect to each parameter (this is called a partial derivative) by holding the inputs and all other weights constant. This is possible because all the neural net functions we've used (linear transforms and activations) are differentiable. We'll talk more about how the derivative is computed later (it uses a cool technique called \"back propogation\" which is based on the chain rule), but for now assume we can compute it efficiently. We then adjust the weights slightly in the negative direction of the derivative. We do this again and again for multiple training batches and that process is called gradient descent. Each step we adjust the weight by the gradient times a small number called the learning rate. Here's what it might look like for one parameter:\n",
    "<center><img src=\"images/one_parameter_gradient_descent.png\"/></center>\n",
    "\n",
    "\n",
    "Here's what it might look like for two parameters. \n",
    "<center><img src=\"images/two_parameter_gradient_descent.png\"/></center>\n",
    "Many people ask; can this get stuck in a local minima? In theory, yes. But in practice, no. That's because in higher dimension spaces (our simple network has 12k dimensions, chatGPT has hundreds of billions) we don't see local minima in practice - just saddle points.\n",
    "\n",
    "The problem with a vanilla gradient descent optimizer is that if the training data is large and you have to processes in batches, it can bounce around too much; it's *stochastic* as each batch of training data moves it in a slightly different direction, and can sometimes overshoot the minima and have to come back if the learning rate is too high (and take forever to train if the learning rate is too low). Optimizers like **adam** build on sgd and add techniques like \"momentum\" (averaging the newly computed parameter updates with the previous ones, so things go in a more straight line):\n",
    "<center><img src=\"images/stochastic_gradient_descent.png\"/></center>\n",
    "\n",
    "With that in mind, here's the updated code; hopefully you can now understand the last two lines:\n",
    "* The optimizer is Adam - an optimized gradient descent algorithm\n",
    "* The loss function is cross-entropy - measuring the expected vs predicted outputs for categorical predictions (in this case, 10 probabilities)\n",
    "* And it trains over all the training data 5 times (5 epochs)\n",
    "\n",
    "```python\n",
    "neural_net = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),          # Input layer\n",
    "    tf.keras.layers.Dense(64, activation='relu'),           # Hidden layer with ReLU activation\n",
    "    tf.keras.layers.Dense(10, activation='softmax')         # Output layer with softmax activation\n",
    "])\n",
    "neural_net.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "neural_net.fit(train_images, train_labels, epochs=5)\n",
    "```\n",
    "\n",
    "As you learned in the intro notebook, this simple network gives over 97% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You try it\n",
    "\n",
    "Frameworks like Keras and PyTorch perform \"auto-grad\"; automatically computing gradients during the forward pass on the training data so that they can adjust the weights for you during the backwards pass. We didn't explicitly specify a learning rate, so got the default. And the fact that gradients were computed and applied were all opaque to us as the framework took care of it.\n",
    "\n",
    "The nice thing about Google's Keras is that it is easier to use for begginers and it was the first one created. However PyTorch is a newer framework by Meta that has started to become dominant because it is more flexible and easier to see under the covers. In the cells below, we'll use PyTorch to see the match behind the training.\n",
    "\n",
    "The network below is super-simple because I just want to show the math in action:\n",
    "1) One neuron; 2 inputs, 1 output\n",
    "2) No activation function\n",
    "3) Training data to teach it to compute f(x1, x2) = 2x1 + x2 + 1\n",
    "Since it's a regression model, we'll use MSE loss.\n",
    "\n",
    "Not might be a good time to talk about how backprop works.\n",
    "* Our simple neural net has 3 parameters, w1, w2 and b, and computes f(x1, x2) = w1*x1 + w2*x2 + b\n",
    "* We are using MSE loss; l(x, y) = (x-y)<sup>2</sup>\n",
    "\n",
    "So after passing in training data X=(x1,x2) and expecting output y, then in order to figure out how to adjust the parameter w1 (for example), we need to compute *d/dw1* l(f(X), y). Let's see how that is done.\n",
    "\n",
    "Recall during the forward pass, the data went through two steps (in a more complex network there would be more steps):\n",
    "1) X -> f(X)           - the linear layer\n",
    "2) f(X) -> l(f(X), y)  - the loss function\n",
    "\n",
    "As this happened, the framework kept track of the computational graph, as well as the input and output at each step.\n",
    "\n",
    "To perform backprop and adjusts the weights:\n",
    "1. By the chain rule: *d/dw1* l(f(X), y) = *dl/dw1*(f(X), y) *df/dw1*(X).\n",
    "2. But *dl/dw1*(f(X), y) is just 2(f(X)), since it's MSE. And the framework kept track of the the input f(X)=w1*x1 + w2*x2 + b during the forward pass, so it doesn't need to be recomputed.\n",
    "3. *df/dw1*(X) is also easy to calculate. It's just x1! And again the framework kept track of x1 during the forward pass.\n",
    "4. So *d/dw1* l(f(X), y) is just the products of the partial derivatives computed in the previous two steps. And since w1 is defined in this layer, it's value is updated by it's gradient times a small learning rate.\n",
    "\n",
    "In summary; the derivative of the last function (the loss function) is computed first, then iteratively the derivative at each point in the computational graph is backproped so it can be multiplied by the derivative of the function at the previous layer. Along the way, weights are adjusted at each node in the graph.\n",
    "\n",
    "\n",
    "OK; let's see this in action...\n",
    "\n",
    "## But first, clone this notebook in your Google Colab account\n",
    "Sign in to [Google colab](https://colab.research.google.com) and from there:\n",
    "1. Go to \"Open Notebook\" (it should have popped up by default, but if not you can find it under the \"File\" menu)\n",
    "2. Select \"GitHub\"\n",
    "3. Enter the following path: https://github.com/marcshepard/teaching-data-science/blob/main/neural-net-math-overview.ipynb\n",
    "4. Click the search icon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's create a model, some training data, and a few support functions.\n",
    "# This is a bit verbose since it uses PyTorch, so don't worry too much about the details...\n",
    "\n",
    "!pip install torchviz\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchviz import make_dot\n",
    "from IPython.display import Image\n",
    "\n",
    "# Generate some training data; f(x1, x2) = 2*x1 + x2 + 1\n",
    "x_train = torch.randn(1000, 2)                      # Inputs: 1000 pairs of random numbers\n",
    "y_train = (2*x_train[:, 0] + x_train[:, 1] + 1)     # Outputs: y = 2*x1 + x2 + 1\n",
    "\n",
    "# Define the model, optimizer, and loss function\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = nn.Linear(2, 1)   # 1 neuron with 2 inputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "model = Model()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Function to show the models parameters - there should be 3: 2 weights and 1 bias\n",
    "def print_parameters():\n",
    "    print(\"Model parameters:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name, param.data)\n",
    "\n",
    "# A function to evaluate the model's loss when run on the training data\n",
    "def evaluate_loss():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(x_train)\n",
    "        loss = loss_fn(outputs.view(-1), y_train)\n",
    "    return loss.item()\n",
    "\n",
    "# A function to train the model for some number of epochs\n",
    "def train(epochs):\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_train)\n",
    "        loss = loss_fn(outputs, y_train.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, let's look at what happens to the parameters before and after 1 training epoch and visualize the computation graph\n",
    "\n",
    "print (\"We're training a 1 neural network with 2 inputs to approximate the function f(x1, x2) = 2*x1 + x2 + 1\")\n",
    "print (\"We expect it to converge to weights of 2 and 1 and a bias of 1 after enough training epochs\")\n",
    "\n",
    "print (\"Before training:\")\n",
    "print_parameters()\n",
    "print (\"\\nMSE loss on the training data:\", evaluate_loss())\n",
    "\n",
    "print()\n",
    "print(\"After training for 1 epoch:\")\n",
    "train(1)\n",
    "print_parameters()\n",
    "print (\"\\nMSE loss on the training data:\", evaluate_loss())\n",
    "\n",
    "# Let's visualize the computation graph associated with the loss\n",
    "loss = loss_fn(model(x_train), y_train.view(-1, 1))\n",
    "make_dot(loss, params=dict(model.named_parameters())).render(\"graph\", format=\"png\")\n",
    "Image(filename='graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you try it a few times. Watch things get better as you increase the number of epochs!\n",
    "\n",
    "epochs = 100 # TODO, change this as you see fit, or just run this cell multiple times to get it to converge\n",
    "\n",
    "print(\"After training for {epochs} epoch:\")\n",
    "train(epochs)\n",
    "print_parameters()\n",
    "print (\"\\nMSE loss on the training data:\", evaluate_loss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To learn more\n",
    "\n",
    "If you are interested in learning more about the mathematics behind neural network training, Andrej Karpathy's series [Neural Networks: Zero to Hero](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ) is an excellent resource."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
